{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word vector notebook\n",
    "\n",
    "This notebook enables you to create word vectors from corpora on TDM Studio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Locating NLTK data\n",
    "You have to tell the system where to find the data it'll need to be able to use NLTK to tokenize the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "nltkpath = os.getcwd() + '/nltk_data'\n",
    "nltk.data.path.append(nltkpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting the source texts\n",
    "\n",
    "In TDM Studio, the datasets you have created are in the `data` folder, in a subfolder with the name of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning the source texts\n",
    "\n",
    "For better performance, the text should be tokenized and lower-cased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Importing modules and setting up paths\n",
    "Change the value of `sourcefiledirectory` to where your dataset is, then run the code block below first, even if you want to move on immediately to the word vectors. It imports a number of modules you'll need later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os is used for things like changing directories and listing files\n",
    "import os\n",
    "#io is used for opening and writing files\n",
    "import io\n",
    "#itertools is used for some of the iterative code\n",
    "from itertools import chain\n",
    "#glob is used to find all the pathnames matching a specified pattern (here, all text files)\n",
    "import glob\n",
    "#pandas is used to extract the data from the xml files and turn them into a table\n",
    "import pandas as pd\n",
    "#lxml is used to parse the xml files\n",
    "from lxml import etree\n",
    "#bs4 is used to parse the xml files\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "#This is the full path to the directory where you've stored the source texts. Be sure to keep the slash at the end when you put in your actual dataset name\n",
    "sourcefiledirectory = 'data/dataset-name/'\n",
    "dataset_directory = sourcefiledirectory\n",
    "input_files = os.listdir(sourcefiledirectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the data\n",
    "The following code extracts the actual article content from each of the articles in your data set, and adds it to a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getxmlcontent(root):\n",
    "    if root.find('.//HiddenText') is not None:\n",
    "        return(root.find('.//HiddenText').text)\n",
    "    elif root.find('.//Text') is not None:\n",
    "        return(root.find('.//Text').text)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creates three lists to store filename, full text, and date\n",
    "\n",
    "filename_list = []\n",
    "text_list = []\n",
    "date_list = []\n",
    "newspaper_list = []\n",
    "\n",
    "#Parse file and add data to lists\n",
    "\n",
    "for file in input_files:\n",
    "    tree = etree.parse(dataset_directory + file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    if getxmlcontent(root) is not None:\n",
    "        soup = BeautifulSoup(getxmlcontent(root))\n",
    "        text = soup.get_text()\n",
    "    else:\n",
    "        text = 'Error in processing document'\n",
    "    \n",
    "    date = root.find('.//NumericDate').text\n",
    "    newspaper = root.find('.//PubFrosting//Title').text\n",
    "\n",
    "    filename_list.append(file)\n",
    "    text_list.append(text)\n",
    "    date_list.append(date)\n",
    "    newspaper_list.append(newspaper)\n",
    "\n",
    "#creates table\n",
    "df = pd.DataFrame({'Article ID': filename_list, 'Newspaper': newspaper_list, 'Date': date_list, 'Text': text_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize text\n",
    "The following code tokenizes and lowercases the text. It then exports each tokenized text as a file in a new directory called `tokenized`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df['Tokenized'] = df['Text'].apply(lambda x: word_tokenize(x))\n",
    "df['Tokenized'] = df['Tokenized'].apply(lambda x: ' '.join(x))\n",
    "df['Tokenized'] = df['Tokenized'].str.lower()\n",
    "\n",
    "isExist = os.path.exists('tokenized')\n",
    "if not isExist:\n",
    "    os.mkdir('tokenized')\n",
    "os.chdir('tokenized')\n",
    "for index, row in df.iterrows():\n",
    "    filename = f\"{index}.txt\"\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(row['Tokenized'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word vector creation\n",
    "\n",
    "The code blocks in this section generate the word vector representation for a set of texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim is a Python module for generating and analyzing word vectors\n",
    "import gensim\n",
    "# Logging allows you to watch the progress of long-running processes\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# word2vec is used to generate the vectors, phrases to identify phrases as an input for vector generation\n",
    "from gensim.models import word2vec, Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "# These utilities are used for exporting and loading models\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "vector_sources=\".\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 List each file and its length\n",
    "This is a confirmation step that lists all the files that will be used as the input for the word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change directory to where the data for your word vectors is\n",
    "os.chdir(vector_sources)\n",
    "#List all the documents in the directory with the data for your word vectors\n",
    "documents = list()\n",
    "for filename in glob.glob(\"*.txt\"):\n",
    "    #Open each text file in the directory and read it into a string\n",
    "    f = io.open(filename, mode=\"r\", encoding=\"utf-8\")\n",
    "    filedata = f.read()\n",
    "    #Print the filename along with how many characters (i.e. letters, numbers, etc.) are in the file\n",
    "    print(filename + \" = \" + str(len(filedata)) + \" chars\")\n",
    "    documents = documents + filedata.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Identify phrases\n",
    "This code block identifies bigram and trigram (2-word and 3-word, respectively) phrases. Phrases are treated like single words when doing the word vector generation. **Note:** this will take some time, and will generate a lot of status messages in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates bigrams and trigrams from the text\n",
    "sentence_stream = [doc.split(\" \") for doc in documents]\n",
    "trigram_sentences_project = []\n",
    "bigram = Phraser(Phrases(sentence_stream))\n",
    "trigram = Phraser(Phrases(bigram[sentence_stream]))\n",
    "\n",
    "for sent in sentence_stream:\n",
    "    bigrams_ = bigram[sent]\n",
    "    trigrams_ = trigram[bigram[sent]]\n",
    "    trigram_sentences_project.append(trigrams_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Running and saving word vectors\n",
    "This code block sets the parameters for vector generation, generates vectors, and saves the model. **Note:** this will take some time and generate a lot of status messages in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets values for various parameters for vector generation.\n",
    "num_features = 200    # Word vector dimensionality                      \n",
    "min_word_count = 2    # Minimum word count                        \n",
    "num_workers = 20      # Number of threads to run in parallel\n",
    "context = 5           # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "\n",
    "# Sets up the code to run the word vector creation\n",
    "model = word2vec.Word2Vec(trigram_sentences_project, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "\n",
    "# Saves model; you can change the name as long as it ends in .model\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the total number of items (including words, phrases, standalone punctuation, etc.) in the model's vocabulary\n",
    "\n",
    "print(len(model.wv.index_to_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word vector analysis\n",
    "\n",
    "The code blocks below allow you to pull up most-similar and most-dissimilar terms, and attempt analogies with the word vectors. (The \"Harry Potter\" and \"Tanya Grotter\" corpora-- even combined-- don't seem to be large enough to support meaningful analogies, but you may be able to train the models further on fanfic to get there.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Most similar terms\n",
    "Put any word in the corpus between the quotes below to show the most similar words. You can change the value of *topn* to show more, or fewer, words.\n",
    "\n",
    "Keep in mind that if you used the preprocessing steps, the text is all lower-case and lemmatized, so no capital letters or inflected forms or else it will throw an error about the word not being in the vocabulary.\n",
    "\n",
    "If you want more words, change `topn` to a higher number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = \"pilot\"\n",
    "model.wv.most_similar (positive=w1,topn=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Analogies\n",
    "Without a larger corpus, the results of these analogies is very dissatisfying. The code below shows how to construct these analogies if you want to try them.\n",
    "\n",
    "The analogy code takes three words as input. To render the analogy pilot:plane::sailor:??? (one might imagine boat as a high probability answer), you would use the code below. Or, more abstractly, given *A:B::C:??*, the code would be: `positive=['A','C'],negative=['B']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pilot is to plane what sailor is to...\n",
    "model.wv.most_similar(positive=['pilot','sailor'],negative=['plane'],topn=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
